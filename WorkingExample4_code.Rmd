---
title: "WORKING EXAMPLE 4"
subtitle:  "In: A guide to test association between Polygenic Risk Scores and psychological and psychiatric traits: practical examples"
author: "Itziar Irigoien, Patricia Mas, Sergi Papiol, Neus Barrantes-Vidal, Araceli Rosa, and Concepción Arenas"
output:
  pdf_document:
    keep_tex: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
source("Functions.R")
library(ggplot2)
library(car)
library(multcomp)
```


## Working flow and code

In this example we simulate 10 PRSs, and a binary trait, with gender, clinical diagnosis (with 2 categories), age, and two Principal Components as covariates. 

- data reading
```{r}
dat <- read.table("WExample4.csv", header=TRUE, sep=";", dec=",")
names(dat) #
```

\bigskip

- do not forget to declare the categorical variables as factors
```{r}
dat$Sex <- as.factor(dat$Sex)
dat$Diagnostic <- as.factor(dat$Diagnostic)
dat$Trait <- as.factor(dat$Trait)
```

## 1.	What full model should be considered?

First, given a particular PRS (named PRS.i), consider all the possible full models:


- FM$_{WI}$: log(p/1-p) versus PRS.i + Sex + Diagnostic + Age + PC1 + PC2
- FM$_{Sex}$: log(p/1-p) versus PRS.i + Sex + PRS.i ·Sex + Diagnostic + Age + PC1 + PC2
- FM$_{Diagnostic}$: log(p/1-p) versus PRS.i + Sex + Diagnostic +    PRS.i · Diagnostic + Age + PC1 + PC2
- FM$_{Sex/Diagnostic}$: log(p/1-p) versus PRS.i + Sex + PRS.i · Sex + Diagnostic + PRS.i · Diagnostic + Age + PC1 + PC2

## 2. How to make a PRS ranking to find the important ones?

As is described in the paper, for each model, calculate the Tjur's coefficients of discrimination. If Nagelkerke's $R^2$ is prefered, set statistic="PseudoR2" in function  orderBin(), and calculate their sum, $S$. 

According to S, list the PRSs in decreasing order:
```{r}
# Order the PRSs
out <- orderBin(dat, yname="Trait", prsname = "PRS.", statistic = "D")
head(out)
mainfilename <- "WExample4"
filename <- paste0(mainfilename, "_Ordered_PRS.csv")
write.csv2(out,file=filename)
```

Plot the sum of coefficients of discrimination coefficients  $D$. Lines: in blue the median; in black the mean.
```{r}
out <- data.frame(out) 
n <- dim(out)[1]
select <- grep("Model", names(out), value=FALSE)
out$effect <- out$Sum
sds <- apply(out[, select], 1, sd)
out$lower <- out$effect - sds
out$upper <- out$effect + sds
out$rank <- n:1


ggplot(data=out, aes(y=rank, x=effect, xmin=lower, xmax=upper)) +
  geom_point() +
  geom_errorbarh(height=.1) +
  scale_y_continuous(name=NULL, breaks= n:1, labels=row.names(out), position="right") +
  labs(title='', x='Sum of D', y = 'PRS') +
  geom_vline(xintercept=mean(out$effect), color='black', linetype='dashed') +
  geom_vline(xintercept=median(out$effect), color='blue', linetype='dashed') +
  theme_minimal()

```

According to the obtained results, first PRS.7 is selected to analyse its association with the Trait.

### 3. Which model, of all the possible ones, should be used?

The following figure represents the scatter plot separated by Sex and Diagnostic groups.
```{r}
# First candidate PRS.7
# Plot it
M <- glm(Trait ~ PRS.7*Sex + PRS.7*Diagnostic + Age + PC1 + PC2, data=dat, family=binomial())
pre <- M$fitted.values #predict(M,type='response')   
ggplot(dat, aes(x=PRS.7, y=log(pre/(pre+1)))) +
  geom_point(aes(color=Trait)) +
  geom_smooth(method=lm, se=FALSE)+
  facet_grid(Sex ~ Diagnostic, labeller=label_both)
# Candidate FM Trait ~ PRS + Sex + Diagnostic + PRS*Diagnostic + C1 +C2
```

The plots suggest that the interaction between the PRS.7 and the diagnostic is relevant. Thus, we set the full model candidate (FM): $log(p/1-p) \sim PRS + Sex + Diagnostic + PRS \cdot Diagnostic + Age + PC1 +PC2$.

### 5.	For a binary trait, what steps should be followed for a correct analysis?

Check for overdispersion
```{r}
#model
FM <- glm(Trait ~ Sex + PRS.7*Diagnostic + Age + PC1 + PC2, data=dat, family=binomial())
#Residual Deviance
FM$deviance
# Ratio 
FM$deviance/FM$df.residual

```

Since this ratio is close to 1, there is not evidence of overdispersion.

```{r}
#With chi-squared test
FM.od <- glm(Trait ~ Sex + PRS.7*Diagnostic + Age + PC1 + PC2, data=dat, family=quasibinomial())
pchisq(summary(FM.od)$dispersion * FM$df.residual,
       FM$df.residual, lower = FALSE)
```

With this p-value = 0.3389, we conclude that there is not evidence of overdispersion.

Based on the following table...
```{r}
summary(FM)
```

...the results show that PRS.7 is related with the log(p/1-p) in the following way:

  - $\widehat{log(p/1-p)} = 1.953 + 0.008\times PRS.7 + 0.446\times Sex + 0.044\times Age - 5.566\times PC1 + 3.785\times PC2$, if Diagnostic = 0.
  
  - $\widehat{log(p/1-p)} = (1.953+16.714) + (0.008+0.162)\times PRS.7 + 0.446\times Sex + 0.044\times Age - 5.566\times PC1 + 3.785\times PC2$, if Diagnostic = 1.


Check whether the respective PRS coefficients under each group are significant or not.
```{r}
summary(glht(FM, "PRS.7 = 0"))
summary(glht(FM, "PRS.7  + PRS.7:Diagnostic1 = 0"))
```

That means that for those with Diagnostic=0, it seems that the PRS.7 is not related to the Trait with odds = $\exp(0.0082) = 1.008$, but for those with Diagnosis =1 the model indicates that the coefficient of PRS.7 is 0.0082 + 0.162 = 0.1702, so the odds increase $\exp(0.1702) = 1.186$ for an incremental of one unit in PRS.7 with a p-value=0.0439.


It is also possible to compute a permutation test to assess whether the increase in the coefficient of determination $D$ is significative.
```{r}
# Null model
NM <- glm(Trait ~  Sex + Diagnostic + Age + PC1 + PC2, data=dat, family=binomial() )
permtest <- dD(NM, FM, seed=1236)
permtest
```

In this particular case, it can be seen that the coefficient of discrimination of the FM model is 0.07 units bigger than the corresponding to the *Null Model* NM, but it is not statistically significant.

- **Last step: We move to the next PRS.**
